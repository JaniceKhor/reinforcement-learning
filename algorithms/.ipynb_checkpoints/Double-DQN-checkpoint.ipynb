{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import math\n",
    "from itertools import count\n",
    "import pdb\n",
    "import warnings\n",
    "\n",
    "from visualize_helper import *\n",
    "from stock_environment import *\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500 #200 \n",
    "BATCH_SIZE = 100 #20\n",
    "GAMMA = 0.97\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 1e-3 # for soft update of target parameters\n",
    "LR = 5e-4\n",
    "PRINT_EVERY = 5\n",
    "UPDATE_EVERY = 4\n",
    "FC1_UNITS = 100\n",
    "FC2_UNITS = 100\n",
    "\n",
    "NUM_EPISODES = 50\n",
    "REMARK  = 'env_2_buffer_500_batch_100_episode_50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../df_apple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_split = '2018-01-02'\n",
    "\n",
    "plot_train_test(df, date_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "train_data = df[df['year']<=2017]\n",
    "valid_data = df[df['year']>2017]\n",
    "\n",
    "train_data = train_data.drop(['year'],axis=1)\n",
    "valid_data = valid_data.drop(['year'],axis=1)\n",
    "\n",
    "print('Train Size: ', train_data.shape[0])\n",
    "print('Valid Size: ', valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Environment\n",
    "\n",
    "Environment information have been written in *stock_environment.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment2(train_data)\n",
    "print(env.reset())\n",
    "for _ in range(10):\n",
    "    pact = np.random.randint(3)\n",
    "    print('action: ', pact)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Define DQN architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1_layer = nn.Linear(state_size, fcl_units)\n",
    "        self.fc2_layer = nn.Linear(fcl_units, fc2_units)\n",
    "        self.fc3_layer = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state into action values.\"\"\"\n",
    "        \n",
    "        hidden = F.relu(self.fc1_layer(state))\n",
    "        hidden = F.relu(self.fc2_layer(hidden))\n",
    "        Qsa = self.fc3_layer(hidden)\n",
    "        \n",
    "        return Qsa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([exp.state for exp in experiences if exp is not None])).float()\n",
    "        states = states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize an agent object.\"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network\n",
    "        self.dqn_net = DQN(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "        self.target_net = DQN(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "        self.target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.dqn_net.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # if enough samples are available in memory, get random subset and learn\n",
    "        if len(self.buffer) > BATCH_SIZE:\n",
    "            experiences = self.buffer.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "            \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        state = torch.FloatTensor(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.dqn_net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.dqn_net(state)\n",
    "        self.dqn_net.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy()) # exploit\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # explore\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        ##### Difference between DQN and DDQN is here #####\n",
    "        \n",
    "        # get the action indices of max predicted Q values (for next states) from policy network\n",
    "        Q_next_best_action = self.dqn_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        \n",
    "        #print('Q_next_best_action_indices: ', Q_next_best_action_indices)\n",
    "        \n",
    "        # use target network to get the Q values (for next states) with that action \n",
    "        Q_target_next = self.target_net(next_states).detach().gather(1, Q_next_best_action)\n",
    "        \n",
    "        #print('Q_next: ', Q_next)\n",
    "        #print('Q_target_next: ', Q_target_next)\n",
    "        \n",
    "        # compute Q target\n",
    "        #### original Q_target\n",
    "        # Q_target = rewards + (gamma * Q_target_next) \n",
    "        \n",
    "        # if current state is end of episode, then there is no next Q value\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones)) \n",
    "        \n",
    "        #print('Q_target: ', Q_target)\n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.time_step = (self.time_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if self.time_step == 0:\n",
    "        \n",
    "            # update target network\n",
    "            self.soft_update(self.dqn_net, self.target_net, TAU)\n",
    "        \n",
    "    def soft_update(self, dqn_net, target_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for dqn_param, target_param in zip(dqn_net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for time_step in range(200):\n",
    "    \n",
    "    # select an action\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes, remark='default', max_time=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, save=True):\n",
    "    \"\"\"Train DDQN agent.\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    steps_done = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for time_step in range(max_time):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            elapsed_time = time.time()-start\n",
    "            print(f'Episode: {i_episode}, Average Score: {avg_score:.2f}, Elapsed Time: {elapsed_time:.3f}')\n",
    "            start = time.time()\n",
    "        \n",
    "    print('Training completed.')\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('./agents/'): \n",
    "            os.makedirs('./agents/')\n",
    "        torch.save(agent.dqn_net.state_dict(), f'./agents/DDQN_{remark}.pth')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=NUM_EPISODES, remark=REMARK, max_time=len(env.data)-1, \n",
    "                     eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_reward(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from randomized action\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, random=True, algorithm_name='Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from trained agent\n",
    "\n",
    "model_path = f'./agents/DDQN_{REMARK}.pth'\n",
    "\n",
    "agent = DDQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS)\n",
    "agent.dqn_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, \n",
    "            random=False, algorithm_name='DDQN', agent=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference for modification:**\n",
    "\n",
    "https://github.com/dxyang/DQN_pytorch/blob/master/learn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
