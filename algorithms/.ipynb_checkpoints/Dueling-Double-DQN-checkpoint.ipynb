{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import math\n",
    "from itertools import count\n",
    "import pdb\n",
    "import warnings\n",
    "\n",
    "from visualize_helper import *\n",
    "from stock_environment import *\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current implementation for experiments <br>\n",
    "**if FC=True:** <br>\n",
    "FC1_UNITS = 100 <br>\n",
    "FC2_UNITS = 100 <br>\n",
    "\n",
    "**if FC=False:** <br>\n",
    "FC1_UNITS = 32 <br>\n",
    "FC2_UNITS = 64 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500 #200\n",
    "BATCH_SIZE = 100 #20\n",
    "GAMMA = 0.97\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 1e-3 # for soft update of target parameters\n",
    "LR = 5e-4\n",
    "PRINT_EVERY = 5\n",
    "UPDATE_EVERY = 4\n",
    "FC = False # FC or CONV?\n",
    "\n",
    "if FC:\n",
    "    FC1_UNITS = 100\n",
    "    FC2_UNITS = 100\n",
    "else:\n",
    "    FC1_UNITS = 32\n",
    "    FC2_UNITS = 64\n",
    "\n",
    "NUM_EPISODES = 400\n",
    "REMARK  = 'env_2_buffer_500_batch_100_episode_400_conv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../df_apple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_split = '2018-01-02'\n",
    "\n",
    "plot_train_test(df, date_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "train_data = df[df['year']<=2017]\n",
    "valid_data = df[df['year']>2017]\n",
    "\n",
    "train_data = train_data.drop(['year'],axis=1)\n",
    "valid_data = valid_data.drop(['year'],axis=1)\n",
    "\n",
    "print('Train Size: ', train_data.shape[0])\n",
    "print('Valid Size: ', valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce environment\n",
    "\n",
    "Environment information have been written in *stock_environment.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment2(train_data)\n",
    "print(env.reset())\n",
    "for _ in range(10):\n",
    "    pact = np.random.randint(3)\n",
    "    print('action: ', pact)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling Double DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQN_FC(nn.Module):\n",
    "    \"\"\"Define Dueling Double DQN architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        \n",
    "        super(DDDQN_FC, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1_layer = nn.Linear(state_size, fcl_units)\n",
    "        self.fc2_layer = nn.Linear(fcl_units, fc2_units)\n",
    "        self.fc3_state_value = nn.Linear(fc2_units, fc2_units//2) # for state-value\n",
    "        self.fc3_advantage_value = nn.Linear(fc2_units, fc2_units//2) # for state-dependent action advantages\n",
    "        self.state_value = nn.Linear(fc2_units//2, 1)\n",
    "        self.advantage_value = nn.Linear(fc2_units//2, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state into action values.\"\"\"\n",
    "        \n",
    "        hidden = F.relu(self.fc1_layer(state))\n",
    "        hidden = F.relu(self.fc2_layer(hidden))\n",
    "        hidden_state = F.relu(self.fc3_state_value(hidden))\n",
    "        hidden_advantage = F.relu(self.fc3_advantage_value(hidden))\n",
    "        \n",
    "        state_value_output = self.state_value(hidden_state)\n",
    "        advantage_value_output = self.advantage_value(hidden_advantage)\n",
    "        \n",
    "        Q_value = state_value_output + (advantage_value_output - advantage_value_output.mean())\n",
    "        \n",
    "        return Q_value\n",
    "    \n",
    "\n",
    "class DDDQN_CONV(nn.Module):\n",
    "    \"\"\"Define DQN architecture.\"\"\"   \n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        super(DDDQN_CONV, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.acion_size = action_size\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(self.state_size, fcl_units, kernel_size=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(fcl_units, fc2_units, kernel_size=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(fc2_units, fc2_units, kernel_size=1, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.conv_net(state)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        state_value_output = self.value_stream(features)\n",
    "        advantage_value_output = self.advantage_stream(features)\n",
    "        \n",
    "        Qsa = state_value_output + (advantage_value_output - advantage_value_output.mean())\n",
    "        return Qsa\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv_net(autograd.Variable(torch.zeros(1, self.state_size, 1))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([exp.state for exp in experiences if exp is not None])).float()\n",
    "        states = states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units=64, fc2_units=64, fc=True):\n",
    "        \"\"\"Initialize an agent object.\"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.fc = fc\n",
    "        \n",
    "        if self.fc:\n",
    "        \n",
    "            # Q-Network\n",
    "            self.dqn_net = DDDQN_FC(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            self.target_net = DDDQN_FC(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Q-Network\n",
    "            self.dqn_net = DDDQN_CONV(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            self.target_net = DDDQN_CONV(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "        \n",
    "        self.target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.dqn_net.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # if enough samples are available in memory, get random subset and learn\n",
    "        if len(self.buffer) > BATCH_SIZE:\n",
    "            experiences = self.buffer.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "            \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        state = torch.FloatTensor(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        if not self.fc:\n",
    "            state = torch.unsqueeze(state, 2)\n",
    "        \n",
    "        self.dqn_net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.dqn_net(state)\n",
    "        self.dqn_net.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy()) # exploit\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # explore\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        if not self.fc:\n",
    "            states = torch.unsqueeze(states, 2)\n",
    "            next_states = torch.unsqueeze(next_states, 2)\n",
    "\n",
    "        ##### Difference between DQN and DDQN is here #####\n",
    "        \n",
    "        # get the action indices of max predicted Q values (for next states) from policy network\n",
    "        Q_next_best_action = self.dqn_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        \n",
    "        # use target network to get the Q values (for next states) with that action \n",
    "        Q_target_next = self.target_net(next_states).detach().gather(1, Q_next_best_action)\n",
    "        \n",
    "        # compute Q target\n",
    "        #### original Q_target\n",
    "        # Q_target = rewards + (gamma * Q_target_next) \n",
    "        \n",
    "        # if current state is end of episode, then there is no next Q value\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones)) \n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # zero-out the gradients of weights and biases before back-propagation \n",
    "        # so that the gradients are not accumulated\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.time_step = (self.time_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if self.time_step == 0:\n",
    "        \n",
    "            # update target network\n",
    "            self.soft_update(self.dqn_net, self.target_net, TAU)\n",
    "        \n",
    "    def soft_update(self, dqn_net, target_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for dqn_param, target_param in zip(dqn_net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDDQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS, fc=FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for time_step in range(200):\n",
    "    \n",
    "    # select an action\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes, remark='default', max_time=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, fc=True, save=True):\n",
    "    \"\"\"Train DDDQN agent.\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    steps_done = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for time_step in range(max_time):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            elapsed_time = time.time()-start\n",
    "            print(f'Episode: {i_episode}, Average Score: {avg_score:.2f}, Elapsed Time: {elapsed_time:.3f}')\n",
    "            start = time.time()\n",
    "        \n",
    "    print('Training completed.')\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('./agents/'): \n",
    "            os.makedirs('./agents/')\n",
    "        torch.save(agent.dqn_net.state_dict(), f'./agents/DDDQN_{remark}.pth')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=NUM_EPISODES, remark=REMARK, max_time=len(env.data)-1, \n",
    "                     eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY, fc=FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_reward(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from randomized action\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, random=True, algorithm_name='Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from trained agent\n",
    "\n",
    "model_path = f'./agents/DDDQN_{REMARK}.pth'\n",
    "\n",
    "agent = DDDQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS, fc=FC)\n",
    "agent.dqn_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, \n",
    "            random=False, algorithm_name='DDDQN', agent=agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
