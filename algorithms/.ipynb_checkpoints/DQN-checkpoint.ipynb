{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import math\n",
    "from itertools import count\n",
    "import pdb\n",
    "import warnings\n",
    "\n",
    "from visualize_helper import *\n",
    "from stock_environment import *\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current implementation for experiments <br>\n",
    "**if FC=True:** <br>\n",
    "FC1_UNITS = 100 <br>\n",
    "FC2_UNITS = 100 <br>\n",
    "\n",
    "**if FC=False:** <br>\n",
    "FC1_UNITS = 32 <br>\n",
    "FC2_UNITS = 64 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500 #200\n",
    "BATCH_SIZE = 100 #20\n",
    "GAMMA = 0.97\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 1e-3 # for soft update of target parameters. updating only 0.001% of new weights and remaining as old weights.\n",
    "LR = 5e-4\n",
    "PRINT_EVERY = 5\n",
    "UPDATE_EVERY = 4\n",
    "FC = False # FC or CONV?\n",
    "\n",
    "if FC:\n",
    "    FC1_UNITS = 100\n",
    "    FC2_UNITS = 100\n",
    "else:\n",
    "    FC1_UNITS = 32\n",
    "    FC2_UNITS = 64\n",
    "\n",
    "NUM_EPISODES = 400\n",
    "REMARK  = 'env_2_buffer_500_batch_100_episode_400_conv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../df_apple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and valid data for visualization\n",
    "date_split = '2018-01-02'\n",
    "\n",
    "plot_train_test(df, date_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and valid data\n",
    "train_data = df[df['year']<=2017]\n",
    "valid_data = df[df['year']>2017]\n",
    "\n",
    "train_data = train_data.drop(['year'],axis=1)\n",
    "valid_data = valid_data.drop(['year'],axis=1)\n",
    "\n",
    "print('Train Size: ', train_data.shape[0])\n",
    "print('Valid Size: ', valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Environment\n",
    "\n",
    "Environment information have been written in *stock_environment.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment2(train_data)\n",
    "print(env.reset())\n",
    "for _ in range(10):\n",
    "    pact = np.random.randint(3)\n",
    "    print('action: ', pact)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_FC(nn.Module):\n",
    "    \"\"\"Define DQN architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        \n",
    "        super(DQN_FC, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1_layer = nn.Linear(state_size, fcl_units)\n",
    "        self.fc2_layer = nn.Linear(fcl_units, fc2_units)\n",
    "        self.fc3_layer = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state into action values.\"\"\"\n",
    "        \n",
    "        hidden = F.relu(self.fc1_layer(state))\n",
    "        hidden = F.relu(self.fc2_layer(hidden))\n",
    "        Qsa = self.fc3_layer(hidden)\n",
    "        \n",
    "        return Qsa\n",
    "\n",
    "    \n",
    "class DQN_CONV(nn.Module):\n",
    "    \"\"\"Define DQN with convolutional architecture.\"\"\"   \n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        super(DQN_CONV, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.acion_size = action_size\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(self.state_size, fcl_units, kernel_size=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(fcl_units, fc2_units, kernel_size=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(fc2_units, fc2_units, kernel_size=1, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.acion_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.conv_net(state)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        Qsa = self.fc(features)\n",
    "        return Qsa\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv_net(autograd.Variable(torch.zeros(1, self.state_size, 1))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to buffer.\"\"\"\n",
    "        \n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([exp.state for exp in experiences if exp is not None])).float()\n",
    "        states = states.to(device)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([exp.action for exp in experiences if exp is not None])).long()\n",
    "        actions = actions.to(device)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward for exp in experiences if exp is not None])).float()\n",
    "        rewards = rewards.to(device)\n",
    "        \n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state for exp in experiences if exp is not None])).float()\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float()\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units=64, fc2_units=64, fc=True):\n",
    "        \"\"\"Initialize an agent object.\"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.fc = fc\n",
    "        \n",
    "        if self.fc:\n",
    "        \n",
    "            # Q FC Network\n",
    "            self.dqn_net = DQN_FC(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            self.target_net = DQN_FC(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            # Q Conv Network\n",
    "            self.dqn_net = DQN_CONV(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            self.target_net = DQN_CONV(state_size, action_size, seed, fcl_units, fc2_units).to(device)\n",
    "            \n",
    "            \n",
    "        self.target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.dqn_net.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer.\"\"\"\n",
    "        \n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # if enough samples are available in memory, get random subset and learn\n",
    "        if len(self.buffer) > BATCH_SIZE:\n",
    "            experiences = self.buffer.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "            \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        # state is stored in list.\n",
    "        state = torch.FloatTensor(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        #print(state.shape)\n",
    "        \n",
    "        if not self.fc:\n",
    "            state = torch.unsqueeze(state, 2)\n",
    "            #print(state.shape)\n",
    "        \n",
    "        self.dqn_net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.dqn_net(state)\n",
    "        self.dqn_net.train()\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy()) # exploit\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # explore\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        if not self.fc:\n",
    "            states = torch.unsqueeze(states, 2)\n",
    "            next_states = torch.unsqueeze(next_states, 2)\n",
    "        \n",
    "        # get max predicted Q values (for next states) from target network\n",
    "        Q_target_next = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "                \n",
    "        # compute Q target\n",
    "        #### original Q_target ####\n",
    "        # Q_target = rewards + (gamma * Q_target_next) \n",
    "        \n",
    "        # if current state is end of episode, then there is no next Q value\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones)) \n",
    "        \n",
    "        # get expected Q values from dqn network\n",
    "        Q_expected = self.dqn_net(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_target, Q_expected)\n",
    "        \n",
    "        # zero-out the gradients of weights and biases before back-propagation \n",
    "        # so that the gradients are not accumulated\n",
    "        self.optimizer.zero_grad() \n",
    "        \n",
    "        # minimize the loss\n",
    "        loss.backward() # compute gradients of the loss associated to all weights and biases\n",
    "        self.optimizer.step() # update weights and biases based on the computed gradients\n",
    "        \n",
    "        self.time_step = (self.time_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if self.time_step == 0:\n",
    "        \n",
    "            # update target network\n",
    "            self.soft_update(self.dqn_net, self.target_net, TAU)\n",
    "        \n",
    "    def soft_update(self, dqn_net, target_net, tau):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        \n",
    "        for dqn_param, target_param in zip(dqn_net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(tau*dqn_param.data + (1.0-tau) * target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS, fc=FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for time_step in range(200):\n",
    "    \n",
    "    # select an action\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes, remark='default', max_time=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, fc=True, save=True):\n",
    "    \"\"\"Train DQN agent.\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = []\n",
    "    eps = eps_start\n",
    "    steps_done = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for time_step in range(max_time):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            elapsed_time = time.time()-start\n",
    "            print(f'Episode: {i_episode}, Average Score: {avg_score:.2f}, Elapsed Time: {elapsed_time:.3f}')\n",
    "            start = time.time()\n",
    "        \n",
    "    print('Training completed.')\n",
    "    \n",
    "    if save: \n",
    "        if not os.path.exists('./agents/'): \n",
    "            os.makedirs('./agents/')\n",
    "        torch.save(agent.dqn_net.state_dict(), f'./agents/DQN_{remark}.pth')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=NUM_EPISODES, remark=REMARK, max_time=len(env.data)-1, \n",
    "                     eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY, fc=FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_reward(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from randomized action\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, random=True, algorithm_name='Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from trained agent\n",
    "\n",
    "model_path = f'./agents/DQN_{REMARK}.pth'\n",
    "\n",
    "agent = DQNAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS, fc=FC)\n",
    "agent.dqn_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, \n",
    "            random=False, algorithm_name='DQN', agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
