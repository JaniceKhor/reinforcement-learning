{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import math\n",
    "from itertools import count\n",
    "import pdb\n",
    "import warnings\n",
    "\n",
    "from visualize_helper import *\n",
    "from stock_environment import *\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 200 #500\n",
    "BATCH_SIZE = 20 #100\n",
    "GAMMA = 0.97\n",
    "FC1_UNITS = 100\n",
    "FC2_UNITS = 100\n",
    "\n",
    "TAU = 1e-3 # for soft update of target parameters\n",
    "LR = 5e-4\n",
    "PRINT_EVERY = 5\n",
    "UPDATE_EVERY = 5\n",
    "\n",
    "NUM_EPISODES = 100\n",
    "REMARK  = 'env_2_change_update_every_5_for_episode_100'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../df_apple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and valid data for visualization\n",
    "date_split = '2018-01-02'\n",
    "\n",
    "# visualize the data\n",
    "plot_train_test(df, date_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and valid data\n",
    "train_data = df[df['year']<=2017]\n",
    "valid_data = df[df['year']>2017]\n",
    "\n",
    "train_data = train_data.drop(['year'],axis=1)\n",
    "valid_data = valid_data.drop(['year'],axis=1)\n",
    "\n",
    "print('Train Size: ', train_data.shape[0])\n",
    "print('Valid Size: ', valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Environment\n",
    "\n",
    "Environment information have been written in *stock_environment.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment2(train_data)\n",
    "print(env.reset())\n",
    "for _ in range(10):\n",
    "    pact = np.random.randint(3)\n",
    "    print('action: ', pact)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"Define Policy Gradients architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        \n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fc1_layer = nn.Linear(state_size, fcl_units)\n",
    "        self.fc2_layer = nn.Linear(fcl_units, fc2_units)\n",
    "        self.fc3_layer = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state into action probability.\"\"\"\n",
    "        \n",
    "        hidden = F.relu(self.fc1_layer(state))\n",
    "        hidden = F.relu(self.fc2_layer(hidden))\n",
    "        pract = F.softmax(self.fc3_layer(hidden)) # probablity of action \n",
    "        return pract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "    \"\"\"The agent interacting with and learning from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcl_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize an agent object.\"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # policy network\n",
    "        self.policy_net = PolicyNet(self.state_size, self.action_size, seed, fcl_units, fc2_units).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        # state is stored in list.\n",
    "        state = torch.FloatTensor(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        probs = self.policy_net(state)\n",
    "        #print('probs: ', probs)\n",
    "        \n",
    "        distributions = Categorical(probs)\n",
    "        \n",
    "        action = distributions.sample()\n",
    "        \n",
    "        return action, distributions\n",
    "\n",
    "    def update(self, state_pool, action_pool, prob_pool, reward_pool, steps, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    \n",
    "        # Discount reward\n",
    "        running_add = 0\n",
    "        \n",
    "        for i in reversed(range(steps)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                # Action value function - Expected return by taking action a in state s following policy\n",
    "                # Discount future rewards back to the present using gamma\n",
    "                running_add = running_add * gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        # Normalize reward\n",
    "        reward_mean = np.mean(reward_pool)\n",
    "        reward_std = np.std(reward_pool)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            # normalization - compensating for future uncertainty\n",
    "            reward_pool[i] = (reward_pool[i] - reward_mean) / reward_std\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i in range(steps):\n",
    "            state = state_pool[i]\n",
    "            action = Variable(torch.FloatTensor([action_pool[i]])).to(device)\n",
    "            prob = prob_pool[i]\n",
    "            reward = reward_pool[i]\n",
    "            \n",
    "            #### Monte-Carlo Policy Gradient #### \n",
    "            # gradient ascent\n",
    "            loss = -prob.log_prob(action) * reward  # Negtive score function x reward\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "        self.optimizer.step() # update weights and biases based on the computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PGAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for time_step in range(200):\n",
    "    \n",
    "    # select an action with highest probability\n",
    "    action, prob = agent.act(state)\n",
    "\n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes, remark='default', max_time=1000, batch_size=20, update_every=4, gamma=0.99, save=True):\n",
    "   \n",
    "    \"\"\"Train PG agent.\"\"\"\n",
    "    \n",
    "    # Batch History\n",
    "    state_pool = []\n",
    "    action_pool = []\n",
    "    prob_pool = []\n",
    "    reward_pool = []\n",
    "   \n",
    "    \n",
    "    scores = []\n",
    "    scores_window = []\n",
    "    steps_done = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for time_step in range(max_time):\n",
    "            \n",
    "            # select an action with highest probability\n",
    "            action, prob = agent.act(state)\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            state_pool.append(state)\n",
    "            action_pool.append(float(action))\n",
    "            prob_pool.append(prob)\n",
    "            reward_pool.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            steps_done += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Update policy\n",
    "        if i_episode > 0 and i_episode % UPDATE_EVERY == 0:\n",
    "            agent.update(state_pool, action_pool, prob_pool, reward_pool, steps_done, gamma)\n",
    "            \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            elapsed_time = time.time()-start\n",
    "            print(f'Episode: {i_episode}, Average Score: {avg_score:.2f}, Elapsed Time: {elapsed_time:.3f}')\n",
    "            start = time.time()\n",
    "            \n",
    "        state_pool = []\n",
    "        action_pool = []\n",
    "        prob_pool = []\n",
    "        reward_pool = []\n",
    "        steps_done = 0\n",
    "        \n",
    "    print('Training completed.')\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('./agents/'): \n",
    "            os.makedirs('./agents/')\n",
    "        torch.save(agent.policy_net.state_dict(), f'./agents/PG_{remark}.pth')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=NUM_EPISODES, remark=REMARK, max_time=len(env.data)-1, \n",
    "                     batch_size=BATCH_SIZE, update_every = UPDATE_EVERY, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from randomized action\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, random=True, algorithm_name='Random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result from trained agent\n",
    "\n",
    "model_path = f'./agents/PG_{REMARK}.pth'\n",
    "\n",
    "agent = PGAgent(state_size=env.history_t+1, action_size=3, seed=0, fcl_units=FC1_UNITS, fc2_units=FC2_UNITS)\n",
    "agent.policy_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "plot_result(Environment2(train_data), Environment2(valid_data), date_split, \n",
    "            random=False, algorithm_name='PG', agent=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "1. https://github.com/Finspire13/pytorch-policy-gradient-example/blob/master/pg.py\n",
    "2. https://github.com/zafarali/policy-gradient-methods/blob/master/pg_methods/policies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
